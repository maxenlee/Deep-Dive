{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxenlee/Deep-Dive/blob/main/NLTKProcessor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9GJVs9jqbkv"
      },
      "source": [
        "# Project 5 NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1RPWmGZzLgD",
        "outputId": "f79af869-09be-4a7b-d572-cb13053e9937"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==1.4\n",
            "  Downloading scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4) (3.3.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn==1.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoEpAGoM2Jf7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from textblob import TextBlob, Word\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5EcxkQ7r06y"
      },
      "source": [
        "## Importing File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJ7jksZKtISX"
      },
      "outputs": [],
      "source": [
        "# Assuming 'url' is the path to your CSV file\n",
        "csv_url = 'https://ddc-datascience.s3.amazonaws.com/Projects/Project.5-NLP/Data/NLP.csv'\n",
        "df = pd.read_csv(csv_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yCYdqpwBQzK"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVYWBOVUHWLQ"
      },
      "outputs": [],
      "source": [
        "from urllib.parse import unquote\n",
        "df['name'] = df['name'].apply(lambda x: unquote(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPWgD0qYr6ZC"
      },
      "source": [
        "### Transforming the text in the text column\n",
        "\n",
        "\n",
        "*   Tokenizing\n",
        "*   removing punction with REGEX  \n",
        "*   Removing stop words\n",
        "*   lemmatizing\n",
        "*   improved speed with memoization\n",
        "*   caching lemmatization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLPqKwfnXBs7"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import re\n",
        "# import string\n",
        "# import nltk\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# import os\n",
        "# import pickle\n",
        "\n",
        "# # Ensure necessary NLTK data is downloaded\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "# # Initialize global components for text processing\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "# lemmatizer = WordNetLemmatizer()\n",
        "# punctuation_removal_pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
        "\n",
        "# # Load or initialize the lemmatization cache\n",
        "# cache_file_path = 'lemmatization_cache.pkl'\n",
        "# if os.path.exists(cache_file_path):\n",
        "#     with open(cache_file_path, 'rb') as cache_file:\n",
        "#         lemmatization_cache = pickle.load(cache_file)\n",
        "# else:\n",
        "#     lemmatization_cache = {}\n",
        "\n",
        "# def preprocess_text_optimized(text):\n",
        "#     global lemmatization_cache\n",
        "#     text = punctuation_removal_pattern.sub('', text)\n",
        "#     tokens = word_tokenize(text)\n",
        "#     processed_tokens = []\n",
        "#     for word in tokens:\n",
        "#         word_lower = word.lower()\n",
        "#         if word_lower not in stop_words:\n",
        "#             if word_lower in lemmatization_cache:\n",
        "#                 lemma = lemmatization_cache[word_lower]\n",
        "#             else:\n",
        "#                 lemma = lemmatizer.lemmatize(word_lower)\n",
        "#                 lemmatization_cache[word_lower] = lemma\n",
        "#             processed_tokens.append(lemma)\n",
        "#     return \" \".join(processed_tokens)\n",
        "\n",
        "\n",
        "# class TextProcessor:\n",
        "#     def __init__(self, optional_names_dict=None):\n",
        "#         \"\"\"\n",
        "#         Initializes the TextProcessor with an optional dictionary for name removal.\n",
        "\n",
        "#         Args:\n",
        "#             optional_names_dict (dict): Optional. A dictionary mapping indexes to lists of names to remove from the text.\n",
        "#         \"\"\"\n",
        "#         self.optional_names_dict = optional_names_dict or {}\n",
        "\n",
        "#     def preprocess_dataframe(self, data, text_column_name='text'):\n",
        "#         \"\"\"\n",
        "#         Preprocesses a DataFrame by removing specified names and applying text optimization techniques on a specified text column.\n",
        "\n",
        "#         Args:\n",
        "#             df (pd.DataFrame): The DataFrame containing the text to be processed.\n",
        "#             text_column_name (str): The name of the column in the DataFrame that contains the text to process.\n",
        "\n",
        "#         Returns:\n",
        "#             pd.DataFrame: A DataFrame with the processed text in the specified column.\n",
        "#         \"\"\"\n",
        "#         processed_texts = []\n",
        "#         for index, row in data.iterrows():\n",
        "#             text = row[text_column_name]\n",
        "#             # Optional name removal\n",
        "#             if index in self.optional_names_dict:\n",
        "#                 for name in self.optional_names_dict[index]:\n",
        "#                     text = re.sub(r'\\b' + re.escape(name) + r'\\b', '', text, flags=re.IGNORECASE)\n",
        "#             # Apply optimized text processing\n",
        "#             processed_text = preprocess_text_optimized(text)\n",
        "#             processed_texts.append(processed_text)\n",
        "\n",
        "#         # Example place to save the cache after processing is complete\n",
        "#         with open(cache_file_path, 'wb') as cache_file:\n",
        "#             pickle.dump(lemmatization_cache, cache_file)\n",
        "\n",
        "#         # Update the DataFrame with processed texts\n",
        "#         data[text_column_name] = processed_texts\n",
        "#         return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgCwbq09YJJa"
      },
      "outputs": [],
      "source": [
        "# # Assuming 'df' is your DataFrame and 'name' is a column of names you want to optionally remove\n",
        "# # Convert 'name' column to a dictionary mapping from index to a list of names (if there are multiple names to remove per index)\n",
        "# optional_names_dict = df['name'].apply(lambda x: [x.lower()]).to_dict()\n",
        "\n",
        "# # Initialize the TextProcessor with the optional names dictionary\n",
        "# processor = TextProcessor(optional_names_dict=optional_names_dict)\n",
        "\n",
        "# # Process the DataFrame, specifying the text column to process\n",
        "# processed_df = processor.preprocess_dataframe(df, text_column_name='text')\n",
        "# # Assuming 'df' is your original DataFrame and 'text_column_name' is the name of the column containing text\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Ensure that NLTK resources are downloaded (do this once)\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "# Initialize the lemmatizer and stop words list\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Cache for memoization\n",
        "lemmatization_cache = {}\n",
        "\n",
        "def lemmatize_with_cache(word):\n",
        "    # If the word has already been lemmatized, return the result from the cache\n",
        "    if word in lemmatization_cache:\n",
        "        return lemmatization_cache[word]\n",
        "    # Otherwise, lemmatize the word and store the result in the cache\n",
        "    else:\n",
        "        lemmatized_word = lemmatizer.lemmatize(word)\n",
        "        lemmatization_cache[word] = lemmatized_word\n",
        "        return lemmatized_word\n",
        "\n",
        "def get_proper_nouns(text):\n",
        "    \"\"\"Extract proper nouns from a given text.\"\"\"\n",
        "    proper_nouns = []\n",
        "    tagged_tokens = pos_tag(word_tokenize(text))\n",
        "    for word, tag in tagged_tokens:\n",
        "        if tag == 'NNP' or tag == 'NNPS':  # Proper nouns\n",
        "            proper_nouns.append(word.lower())\n",
        "    return set(proper_nouns)\n",
        "\n",
        "def process_text(df,on,optional_dict = {}):\n",
        "    processed_texts = []\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        # Extract proper nouns from the 'name' column to exclude from the 'text'\n",
        "        proper_nouns = get_proper_nouns(row[optional_dict])\n",
        "\n",
        "        # Tokenize and remove punctuation\n",
        "        tokens = word_tokenize(row[on])\n",
        "        tokens = [re.sub(r'\\W+', '', token) for token in tokens if re.sub(r'\\W+', '', token)]\n",
        "\n",
        "        # Remove stop words, proper nouns, and lemmatize\n",
        "        tokens = [lemmatize_with_cache(token.lower()) for token in tokens if token.lower() not in stop_words and token.lower() not in proper_nouns]\n",
        "\n",
        "        # Rejoin tokens into a single string\n",
        "        processed_text = ' '.join(tokens)\n",
        "        processed_texts.append(processed_text)\n",
        "\n",
        "    # Return a new DataFrame with processed texts\n",
        "    return pd.DataFrame({'text': processed_texts})\n"
      ],
      "metadata": {
        "id": "bLZrsBRvQDqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'df' is your DataFrame with a 'text' column\n",
        "processed_df = process_text(df,on = 'text', optional_dict= 'name')\n"
      ],
      "metadata": {
        "id": "gujq58GJQJRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i67fLynbRZ3S"
      },
      "source": [
        "# TF-IDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SX0DwPo0CKbl"
      },
      "outputs": [],
      "source": [
        "processed_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'] = processed_df['text']"
      ],
      "metadata": {
        "id": "s2d2Bg2dARcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text']"
      ],
      "metadata": {
        "id": "SaMi-jmcAXjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "og_size = sum(df['text'].apply(lambda x: len(x)))\n",
        "og_size"
      ],
      "metadata": {
        "id": "7gQjvO1cL2_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_size = sum(processed_df['text'].apply(lambda x: len(x)))\n",
        "processed_size"
      ],
      "metadata": {
        "id": "D-R5jRsPLa58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "og_size - processed_size"
      ],
      "metadata": {
        "id": "8DXJ92zbLpES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfeXe2MbBncL"
      },
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tf_idf_matrix = tfidf_vectorizer.fit_transform(processed_df['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyGjdhzassnS"
      },
      "source": [
        "### Fitting K Nearest neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtYAV-v7BqSY"
      },
      "outputs": [],
      "source": [
        "nearest_neighbors = NearestNeighbors().fit(tf_idf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5ghzGyCCkz6"
      },
      "outputs": [],
      "source": [
        "np.random.seed(24)\n",
        "page = random.randrange(0,len(df))\n",
        "wiki = tf_idf_matrix[page]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjjUZlDTCohE"
      },
      "outputs": [],
      "source": [
        "page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb9S8mAJCyf6"
      },
      "outputs": [],
      "source": [
        "wiki"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05uqHRX_tCpY"
      },
      "source": [
        "## nearest neighbors in DB Pedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ul9-XdDnC3B2"
      },
      "outputs": [],
      "source": [
        "distances, indices = nearest_neighbors.kneighbors(X = wiki , n_neighbors=11 )\n",
        "\n",
        "# Retrieve the original texts for the nearest neighbors\n",
        "# nearest_texts = original_texts.iloc[indices.flatten()]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sg00UcLFC7BY"
      },
      "outputs": [],
      "source": [
        "neighbors_index = list(indices[0][:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZMEqu-dC-8q"
      },
      "outputs": [],
      "source": [
        "neighbors_distance = list(distances[0][:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AR12Vpy9_zRG"
      },
      "outputs": [],
      "source": [
        "zipped = list(zip(neighbors_index,neighbors_distance))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RBs4n3M_7RU"
      },
      "outputs": [],
      "source": [
        "zipped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLurtTkOD5Aw"
      },
      "outputs": [],
      "source": [
        "db_name = df.iloc[page]['name']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMb2kV2nDBvs"
      },
      "outputs": [],
      "source": [
        "df_DBneighbors = df.iloc[neighbors_index]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMq0f5RU_pGV"
      },
      "outputs": [],
      "source": [
        "DB_nn = pd.Series(distances[0][:],name = 'distances' , index= indices[0][:])\n",
        "# df_Wiki_Neighbors = pd.concat([df_Wiki_Neighbors,wiki_nn], axis = 0)\n",
        "df_DBneighbors = df_DBneighbors.join(DB_nn,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KZzp5cszKyG"
      },
      "outputs": [],
      "source": [
        "df_DBneighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHFL7E7WtLHU"
      },
      "source": [
        "### Sentiment scores for each neighbor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGIddL7t0usv"
      },
      "outputs": [],
      "source": [
        "df_DBneighbors.loc[:, 'sentiment'] = df_DBneighbors['text'].apply(lambda x: TextBlob(x).sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyhRFQgJDSz8"
      },
      "outputs": [],
      "source": [
        "df_dict={}\n",
        "df_dict['DB'] = df_DBneighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ABuUxwT3_bs"
      },
      "outputs": [],
      "source": [
        "df_dict['DB']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPSgkpV0QXxh"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiHTXVBGQWt-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ppg5mKGCQmYf"
      },
      "source": [
        "# Part 2: The SQL\n",
        "Of the 10 Nearest Neighbors, Find their Whole Wiki Page and Order Their Indecies by the Whole Page."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "co-PpOL0SHrt"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip3 install wikipedia-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqGmfiD1SK78"
      },
      "outputs": [],
      "source": [
        "import wikipediaapi\n",
        "wikipedia = wikipediaapi.Wikipedia(user_agent = 'abooboo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rrLq5mvu-ki"
      },
      "outputs": [],
      "source": [
        "wiki"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMs92CWKSmYs"
      },
      "outputs": [],
      "source": [
        "Person = db_name\n",
        "person_text = wikipedia.page(Person)\n",
        "person_text.title\n",
        "person_text.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOPGgp7F4IB8"
      },
      "outputs": [],
      "source": [
        "df_Wiki_Neighbors = pd.DataFrame()\n",
        "df_Wiki_Neighbors['name'] = df_DBneighbors['name'].apply(lambda x: wikipedia.page(x).title)\n",
        "df_Wiki_Neighbors['text'] = df_Wiki_Neighbors['name'].apply(lambda x: wikipedia.page(x).text )\n",
        "df_Wiki_Neighbors['sentiment'] = df_Wiki_Neighbors['text'].apply(lambda x: TextBlob(x).sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4ubRx3r4tEN"
      },
      "outputs": [],
      "source": [
        "df_Wiki_Neighbors = df_Wiki_Neighbors.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baWF0_E2xK_u"
      },
      "outputs": [],
      "source": [
        "\n",
        "processed_wiki_df = process_text(df_Wiki_Neighbors,on = 'text', optional_dict= 'name')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Or-QB7Axh3R"
      },
      "outputs": [],
      "source": [
        "processed_wiki_df = processed_wiki_df.reset_index(drop = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saRKxqBkCOvz"
      },
      "outputs": [],
      "source": [
        "processed_wiki_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icLki7H0xhsH"
      },
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tf_idf_matrix = tfidf_vectorizer.fit_transform(processed_wiki_df['text'])\n",
        "nearest_neighbors = NearestNeighbors().fit(tf_idf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKEAUPcC7WHg"
      },
      "outputs": [],
      "source": [
        "page = 0\n",
        "wiki = tf_idf_matrix[page]\n",
        "distances, indices = nearest_neighbors.kneighbors(X = wiki , n_neighbors=11 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHHkYb_k7yt2"
      },
      "outputs": [],
      "source": [
        "distances[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bs_aLJxz7XjS"
      },
      "outputs": [],
      "source": [
        "wiki_nn = pd.Series(distances[0][:],name = 'distances' , index= indices[0][:])\n",
        "# df_Wiki_Neighbors = pd.concat([df_Wiki_Neighbors,wiki_nn], axis = 0)\n",
        "df_Wiki_Neighbors = df_Wiki_Neighbors.join(wiki_nn,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1O-hysN385J8"
      },
      "outputs": [],
      "source": [
        "df_Wiki_Neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part3 Interactivity\n"
      ],
      "metadata": {
        "id": "EjNNbQW4Q9_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Assuming 'df' is your DataFrame and 'name' is a column of names you want to optionally remove\n",
        "# # Convert 'name' column to a dictionary mapping from index to a list of names (if there are multiple names to remove per index)\n",
        "# optional_names_dict = df['name'].apply(lambda x: [x.lower()]).to_dict()\n",
        "\n",
        "# # Initialize the TextProcessor with the optional names dictionary\n",
        "# processor = TextProcessor(optional_names_dict=optional_names_dict)\n",
        "\n",
        "# # Process the DataFrame, specifying the text column to process\n",
        "# processed_df = processor.preprocess_dataframe(df, text_column_name='text')\n",
        "\n",
        "# # 'processed_df' now contains the optimized, processed text"
      ],
      "metadata": {
        "id": "e490IY4WXH2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary imports\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.lines as mlines\n",
        "\n",
        "# Assuming TextProcessor is a custom class you've defined elsewhere\n",
        "# from your_custom_module import TextProcessor\n",
        "\n",
        "def get_nearest_neighbors(df, page, k=10):\n",
        "    \"\"\"\n",
        "    Returns the nearest neighbors from DB Pedia and Wiki for a given page.\n",
        "\n",
        "    Args:\n",
        "      df: The DataFrame containing the processed text.\n",
        "      page: The index of the page to find nearest neighbors for.\n",
        "      k: The number of nearest neighbors to return.\n",
        "\n",
        "    Returns:\n",
        "      A dictionary containing DataFrames of nearest neighbors from DB Pedia and Wiki.\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure the necessary libraries are imported above\n",
        "\n",
        "\n",
        "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tf_idf_matrix = tfidf_vectorizer.fit_transform(df['text'])\n",
        "    nearest_neighbors = NearestNeighbors().fit(tf_idf_matrix)\n",
        "    wiki = tf_idf_matrix[page]\n",
        "    db_distances, db_indices = nearest_neighbors.kneighbors(X=wiki, n_neighbors=k+1)\n",
        "\n",
        "    # DB DataFrame\n",
        "    df_DBneighbors = df.iloc[db_indices[0]]\n",
        "    df_DBneighbors = df_DBneighbors.drop(columns=['URI'], errors='ignore')\n",
        "    df_DBneighbors['sentiment'] = df_DBneighbors['text'].apply(lambda x: TextBlob(x).sentiment)\n",
        "    df_DBneighbors['distances'] = db_distances[0]\n",
        "\n",
        "\n",
        "    # Wiki DataFrame\n",
        "    df_DBneighbors['wiki_name'] = df_DBneighbors['name'].apply(lambda x: wikipedia.page(x).title)\n",
        "    df_DBneighbors['wiki_text'] = df_DBneighbors['wiki_name'].apply(lambda x: wikipedia.page(x).text)  # Use summary for brevity\n",
        "    df_DBneighbors['wiki_sentiment'] = df_DBneighbors['wiki_text'].apply(lambda x: TextBlob(x).sentiment)\n",
        "    wiki_text = process_text(df_DBneighbors,on = 'wiki_text', optional_dict= 'wiki_name')\n",
        "    # Ensure the necessary libraries are imported above\n",
        "\n",
        "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tf_idf_matrix = tfidf_vectorizer.fit_transform(wiki_text['text'])\n",
        "    nearest_neighbors = NearestNeighbors().fit(tf_idf_matrix)\n",
        "    wiki = tf_idf_matrix[0]\n",
        "    db_distances, db_indices = nearest_neighbors.kneighbors(X=wiki, n_neighbors=k+1)\n",
        "    df_DBneighbors['wiki_distances'] = db_distances[0]\n",
        "\n",
        "    # df_DBneighbors\n",
        "\n",
        "    df_DBneighbors['pol1'] = df_DBneighbors['sentiment'].apply(lambda x: x[0])\n",
        "    df_DBneighbors['sub1'] = df_DBneighbors['sentiment'].apply(lambda x: x[1])\n",
        "\n",
        "    df_DBneighbors['pol2'] = df_DBneighbors['wiki_sentiment'].apply(lambda x: x[0])\n",
        "    df_DBneighbors['sub2'] = df_DBneighbors['wiki_sentiment'].apply(lambda x: x[1])\n",
        "\n",
        "    output = df_DBneighbors\n",
        "\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))  # Increase figure width\n",
        "\n",
        "    # Define a list of markers to cycle through\n",
        "    markers = ['o', 'v', '^', '<', '>', 's', 'p', '*', '+', 'x','D']\n",
        "\n",
        "    # Placeholder for custom legend icons\n",
        "    legend_icons = []\n",
        "\n",
        "    mark_i = 0\n",
        "    # Loop through the DataFrame and plot each point with a different marker\n",
        "    for i, row in output.iterrows():\n",
        "        scatter1 = ax.scatter(row['pol1'], row['sub1'], color='blue', label='DB' if i == 0 else \"\", marker=markers[mark_i])\n",
        "        scatter2 = ax.scatter(row['pol2'], row['sub2'], color='red', label='wiki' if i == 0 else \"\", marker=markers[mark_i])\n",
        "        # Create a custom legend icon for each name\n",
        "        legend_icon = mlines.Line2D([], [], color='black', marker=markers[mark_i], linestyle='None', markersize=10, label=row['name'])\n",
        "        legend_icons.append(legend_icon)\n",
        "        mark_i += 1\n",
        "\n",
        "    # Set labels and title\n",
        "    ax.set_xlabel('Polarity')\n",
        "    ax.set_ylabel('Subjectivity')\n",
        "    ax.set_title('Sentiment Analysis: Polarity vs. Subjectivity')\n",
        "\n",
        "    # Create the first legend manually and specify its position outside the plot area\n",
        "    first_legend = ax.legend(handles=legend_icons, loc='upper left', bbox_to_anchor=(1, 1), title='Names')\n",
        "    # Add the first legend manually\n",
        "\n",
        "\n",
        "    second_legend = ax.legend([scatter1, scatter2], ['DB', 'wiki'], loc='lower left', bbox_to_anchor=(1, 0))\n",
        "\n",
        "    # Create and add the second legend for the scatter plot colors/labels, also outside\n",
        "    plt.legend(handles=legend_icons, loc='upper left', bbox_to_anchor=(1, 1), title='Names')\n",
        "    ax.add_artist(second_legend)\n",
        "    plt.subplots_adjust(right=.75)  # Adjust subplot to create more space on the right\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return df_DBneighbors\n",
        "\n",
        "\n",
        "# Example usage (assuming 'df' and 'condition' are defined in your context)\n",
        "search = 'Franz Rottensteiner'\n",
        "condition = df[df['name'] == search].index[0]\n",
        "output = get_nearest_neighbors(df, page=condition)\n"
      ],
      "metadata": {
        "id": "SFx5yyb_5c-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "id": "7QY5MY5zLcwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import matplotlib.lines as mlines\n",
        "\n",
        "# fig, ax = plt.subplots(figsize=(12, 6))  # Increase figure width\n",
        "\n",
        "# # Define a list of markers to cycle through\n",
        "# markers = ['o', 'v', '^', '<', '>', 's', 'p', '*', '+', 'x']\n",
        "\n",
        "# # Placeholder for custom legend icons\n",
        "# legend_icons = []\n",
        "\n",
        "# # Loop through the DataFrame and plot each point with a different marker\n",
        "# for i, row in output.iterrows():\n",
        "#     scatter1 = ax.scatter(row['pol1'], row['sub1'], color='blue', label='DB' if i == 0 else \"\", marker=markers[i % len(markers)])\n",
        "#     scatter2 = ax.scatter(row['pol2'], row['sub2'], color='red', label='wiki' if i == 0 else \"\", marker=markers[i % len(markers)])\n",
        "#     # Create a custom legend icon for each name\n",
        "#     legend_icon = mlines.Line2D([], [], color='black', marker=markers[i % len(markers)], linestyle='None', markersize=10, label=row['name'])\n",
        "#     legend_icons.append(legend_icon)\n",
        "\n",
        "# # Set labels and title\n",
        "# ax.set_xlabel('Polarity')\n",
        "# ax.set_ylabel('Subjectivity')\n",
        "# ax.set_title('Sentiment Analysis: Polarity vs. Subjectivity')\n",
        "\n",
        "# # Create the first legend manually and specify its position outside the plot area\n",
        "# first_legend = ax.legend(handles=legend_icons, loc='upper left', bbox_to_anchor=(1, 1), title='Names')\n",
        "# # Add the first legend manually\n",
        "\n",
        "\n",
        "# second_legend = ax.legend([scatter1, scatter2], ['DB', 'wiki'], loc='lower left', bbox_to_anchor=(1, 0))\n",
        "\n",
        "# # Create and add the second legend for the scatter plot colors/labels, also outside\n",
        "# plt.legend(handles=legend_icons, loc='upper left', bbox_to_anchor=(1, 1), title='Names')\n",
        "# ax.add_artist(second_legend)\n",
        "# plt.subplots_adjust(right=.75)  # Adjust subplot to create more space on the right\n",
        "\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "xJZ5E2qti5Ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# USER INPUT"
      ],
      "metadata": {
        "id": "LLTZgAtl8pMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exact match"
      ],
      "metadata": {
        "id": "iOdt2yY0Zr8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def name_search():\n",
        "  search = input(\"Enter a name from the DataFrame: \")\n",
        "  print('loading...')\n",
        "  condition = df[df['name'] == search].index[0]\n",
        "  output = get_nearest_neighbors(df, page=condition)\n",
        "  print('DING!')"
      ],
      "metadata": {
        "id": "MmnvxCV3R0AM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_neighbors = name_search()"
      ],
      "metadata": {
        "id": "g9-eAA8XSKxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import textdistance\n",
        "\n",
        "\n",
        "def get_closest_names(df, name, k=5):\n",
        "    \"\"\"\n",
        "    Returns the k closest names to the given name in the DataFrame using Levenshtein distance.\n",
        "\n",
        "    Args:\n",
        "        df: The DataFrame containing the names.\n",
        "        name: The name to find the closest names to.\n",
        "        k: The number of closest names to return.\n",
        "\n",
        "    Returns:\n",
        "        A list of the k closest names.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the Levenshtein distance between the given name and all other names\n",
        "    distances = df['name'].apply(lambda x: textdistance.levenshtein(name.lower(), x.lower()))\n",
        "\n",
        "    # Sort the distances in ascending order and get the indices of the k closest names\n",
        "    sorted_indices = np.argsort(distances)[:k]\n",
        "\n",
        "    # Return the names corresponding to the closest indices\n",
        "    return df['name'].iloc[sorted_indices].tolist()\n",
        "\n"
      ],
      "metadata": {
        "id": "jaoN4IUsVaEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usagew\n",
        "search = input(\"Enter a name from the DataFrame: \")\n",
        "closest_names = get_closest_names(df, search)\n",
        "print(\"The 5 closest names to\", search, \"are:\", closest_names)\n"
      ],
      "metadata": {
        "id": "zLCGI8H7W39K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import textdistance\n",
        "\n",
        "# Sample DataFrame\n",
        "# df = pd.DataFrame({'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve']})\n",
        "\n",
        "def get_closest_names(df, name, k=5):\n",
        "    \"\"\"\n",
        "    Returns the k closest names to the given name in the DataFrame using Levenshtein distance.\n",
        "\n",
        "    Args:\n",
        "        df: The DataFrame containing the names.\n",
        "        name: The name to find the closest names to.\n",
        "        k: The number of closest names to return.\n",
        "\n",
        "    Returns:\n",
        "        A list of the k closest names.\n",
        "    \"\"\"\n",
        "    distances = df['name'].apply(lambda x: textdistance.levenshtein(name.lower(), x.lower()))\n",
        "    sorted_indices = np.argsort(distances)[:k]\n",
        "    return df['name'].iloc[sorted_indices].tolist()\n",
        "\n",
        "def execute_function_with_name(search):\n",
        "    \"\"\"\n",
        "    Placeholder for executing a function with the selected name.\n",
        "\n",
        "    Args:\n",
        "        name: The selected name to use in the function.\n",
        "    \"\"\"\n",
        "    print('loading...')\n",
        "    condition = df[df['name'] == search].index[0]\n",
        "    output = get_nearest_neighbors(df, page=condition)\n",
        "    print('DING!')\n",
        "    # Placeholder functionality\n",
        "    print(f\"Function executed with {search}.\")\n"
      ],
      "metadata": {
        "id": "mcebWLovX6ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive process\n",
        "search = input(\"Enter a name from the DataFrame: \")\n",
        "closest_names = get_closest_names(df, search)\n",
        "print(\"The closest names to\", search, \"are:\")\n",
        "for index, name in enumerate(closest_names, start=1):\n",
        "    print(f\"{index}. {name}\")\n",
        "\n",
        "selected_index = int(input(\"Select a name by entering the corresponding number: \")) - 1\n",
        "if 0 <= selected_index < len(closest_names):\n",
        "    selected_name = closest_names[selected_index]\n",
        "    print(f\"You selected: {selected_name}\")\n",
        "    execute_function_with_name(selected_name)\n",
        "else:\n",
        "    print(\"Invalid selection. Please run the process again and select a valid number.\")\n"
      ],
      "metadata": {
        "id": "ti714dXNZzdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m= ['o', 'v', '^', '<', '>', 's', 'p', '*', '+', 'x','^']"
      ],
      "metadata": {
        "id": "RIKhXLpYZRe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 0\n",
        "for i,k in output.iterrows():\n",
        "  print(m[counter])\n",
        "  counter +=1"
      ],
      "metadata": {
        "id": "uM76G2iDcHKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m[0]"
      ],
      "metadata": {
        "id": "QG57UF8XcEIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KKrk9l0mcFOZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHMVbrT8v8M8CP0UKKa457",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}